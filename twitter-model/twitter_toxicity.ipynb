{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# DEPS: \n",
        "\n",
        "%pip install pandas numpy nltk joblib scikit-learn tensorflow snscrape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knuuBvSvdyIB"
      },
      "source": [
        "# Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_folder = '../data/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eDMZOwvT9nmG"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile(data_folder+'twittes_data.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "pcHSDEMheRTi",
        "outputId": "03ca9d4f-c4b9-4d95-b60c-729793377796"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('comentarios_toxicos_ptBR.csv')\n",
        "\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ncvmj-ROoSv7"
      },
      "source": [
        "# Analyzing the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xngkLHU0oSLD",
        "outputId": "accfee2f-f35c-41fa-df5b-79fb8da76276"
      },
      "outputs": [],
      "source": [
        "print('Não Tóxicos: ', len(df[df['toxic'] == 0]))\n",
        "print('Tóxicos: ', len(df[df['toxic'] == 1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gpcd6ObpepbP"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B9vNbxEbf2yD",
        "outputId": "c44f7e90-80dc-4b2d-cad3-95b6d6653983"
      },
      "outputs": [],
      "source": [
        "import re \n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('floresta')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk import word_tokenize\n",
        "import joblib\n",
        "import sys  \n",
        "sys.path.insert(1, data_folder)\n",
        "from abbreviations_synonyms import abbreviations_synonyms_dict\n",
        "\n",
        "tagger = joblib.load(data_folder+'POS_tagger_bigram.pkl') # https://github.com/inoueMashuu/POS-tagger-portuguese-nltk/tree/master/trained_POS_taggers\n",
        "\n",
        "def clean_text(text):\n",
        "  text = ' '.join([ word for word in text.split(' ') if not word.startswith('@') ])\n",
        "  text = re.sub(r\"[^A-Za-z ]+\", '', text) # keep only letters and spaces\n",
        "  text = text.strip()\n",
        "  return text\n",
        "\n",
        "def replace_synonyms_abbreviations(text):\n",
        "  for abbr_or_syn, full_text in abbreviations_synonyms_dict.items():\n",
        "    text = re.sub(rf\"\\b{abbr_or_syn}\\b\",full_text,text)\n",
        "\n",
        "  return text\n",
        "\n",
        "def remove_stop_words(text):\n",
        "  stopwords_pt = stopwords.words('portuguese')\n",
        "  \n",
        "  text_without_sw = [word for word in text.split(' ') if not word in stopwords_pt]\n",
        "  return (\" \").join(text_without_sw)\n",
        "\n",
        "def lemmatization_nltk(text):\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  palavras = nltk.word_tokenize(text, language='portuguese')\n",
        "  lemmas = [lemmatizer.lemmatize(p).lower() for p in palavras]\n",
        "  return (\" \").join(lemmas)\n",
        "\n",
        "def remove_proper_nouns(text):\n",
        "  words = []\n",
        "  for word,tag in tagger.tag(word_tokenize(text)):\n",
        "    if tag != 'NPROP':\n",
        "      words.append(word)\n",
        "  return ' '.join(words)\n",
        "\n",
        "def normalize_text(text):\n",
        "  text = clean_text(text)\n",
        "  text = remove_proper_nouns(text)\n",
        "  text = text.lower() # outside clean_text because capitalization influences remove_proper_nouns function \n",
        "  text = replace_synonyms_abbreviations(text)\n",
        "  text = remove_stop_words(text)\n",
        "  text = lemmatization_nltk(text)\n",
        "  return text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RjcD9PgUhQTP"
      },
      "outputs": [],
      "source": [
        "# our own normalization\n",
        "df = df[df['text'].notna()] # removing nan values\n",
        "df['text_norm2'] = df['text'].apply(normalize_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XrSGX2NZ3mNk",
        "outputId": "81fa238d-fc7e-405e-dbd1-4843f3ad8a2f"
      },
      "outputs": [],
      "source": [
        "df.head(50)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "U9nTuwge5L4m"
      },
      "source": [
        "# Training NN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1aWJNgRRoHrc",
        "outputId": "db902368-712e-41d2-f6d8-776503b7dfe5"
      },
      "outputs": [],
      "source": [
        "# Import functions from sklearn library\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Splitting the data into training and testing sets\n",
        "train_data, test_data = train_test_split(df, test_size=0.2,random_state=16)\n",
        "print(\"Train Data size:\", len(train_data))\n",
        "print(\"Test Data size\", len(test_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XRHGd1rierud",
        "outputId": "ad312695-ff53-493e-9a05-919f5fd40a06"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "tweets_column='text_norm2'\n",
        "\n",
        "tokenizer.fit_on_texts(train_data[tweets_column])\n",
        "word_index = tokenizer.word_index\n",
        "print(word_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gAVOmgNMuEie",
        "outputId": "b8d65230-a606-4126-b449-ddaabc85b2b6"
      },
      "outputs": [],
      "source": [
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(\"Vocabulary Size :\", vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QmEH6iWxuIJe"
      },
      "outputs": [],
      "source": [
        "from keras.utils import pad_sequences\n",
        "\n",
        "# The tokens are converted into sequences and then passed to the pad_sequences() function\n",
        "x_train = pad_sequences(tokenizer.texts_to_sequences(train_data[tweets_column]), maxlen = 30)\n",
        "x_test = pad_sequences(tokenizer.texts_to_sequences(test_data[tweets_column]), maxlen = 30) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUGhjvYau5E_",
        "outputId": "b730d82f-f6c2-4b4e-efed-abe6d7f1866f"
      },
      "outputs": [],
      "source": [
        "y_train = train_data['toxic']\n",
        "y_test = test_data['toxic']\n",
        "\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DOZYX97wjZO-",
        "outputId": "71944b7d-e9e2-4b0c-d482-0e35fe292893"
      },
      "outputs": [],
      "source": [
        "from keras import Sequential\n",
        "from keras.layers import Dense,SimpleRNN,Embedding,Flatten, LSTM, Bidirectional\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, output_dim=2, input_length=30))\n",
        "model.add(SimpleRNN(15,return_sequences=True))\n",
        "model.add(SimpleRNN(15))\n",
        "model.add(Dense(1,activation='sigmoid'))\n",
        "\n",
        "model.summary() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gip63CA9kiOj",
        "outputId": "b9ef3abb-f195-446d-a6e3-55c810c8ddfe"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "history = model.fit(x_train, y_train, epochs=3, validation_data=(x_test,y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Testing MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fC3uvto0DjEt",
        "outputId": "e90e38cb-88b4-400b-92da-2dbcaa014e0a"
      },
      "outputs": [],
      "source": [
        "import snscrape.modules.twitter as sntwitter\n",
        "\n",
        "#recuperando os tweets de uma conta\n",
        "username = \"biologia_braba\"\n",
        "max_tweets = 25\n",
        "\n",
        "tweets = []\n",
        "for i, tweet in enumerate(sntwitter.TwitterSearchScraper(f\"from:{username}\").get_items()):\n",
        "    if i >= max_tweets:\n",
        "        break\n",
        "    tweets.append(tweet)\n",
        "\n",
        "content_tweets = [tweet.rawContent for tweet in tweets]\n",
        "print(content_tweets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tv1doC_DDkCJ",
        "outputId": "a5c9a0ce-7dbc-4008-b21c-669dc3e7a097"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# cálculo de toxicidade \n",
        "tweets_normalized = [normalize_text(tweet) for tweet in content_tweets]\n",
        "tweets_tokens = pad_sequences(tokenizer.texts_to_sequences(tweets_normalized), maxlen = 30)  \n",
        "\n",
        "y = model(tweets_tokens, training=False)\n",
        "\n",
        "for i, raw_tweet in enumerate(tweets_normalized):\n",
        "  print(raw_tweet)\n",
        "  print(content_tweets[i])\n",
        "  print(f\"Toxidade: {float(y[i])*100}%\")\n",
        "  print('------------------------------------------')\n",
        "\n",
        "print(f\"Média de toxicidade do twitter: {np.mean(y)*100}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Export model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import io\n",
        "import json \n",
        "\n",
        "model.save(data_folder+'exported/twitter_toxicity_model')\n",
        "tokenizer_json = tokenizer.to_json()\n",
        "with io.open(data_folder+'exported/tokenizer.json', 'w', encoding='utf-8') as f:\n",
        "  f.write(json.dumps(tokenizer_json, ensure_ascii=False))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "knuuBvSvdyIB",
        "Ncvmj-ROoSv7"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
